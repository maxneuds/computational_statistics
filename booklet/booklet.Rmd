---
output: 
  komadown::scrartcl:
    toc: False
lang: de
includes: 
- \usepackage[utf8]{inputenc}
- \usepackage[T1]{fontenc}
- \usepackage[ngerman]{babel}
- \usepackage{palatino,eulervm,amsmath,amssymb,amsthm}
- \usepackage{dsfont}
- \usepackage{listings}
- \usepackage{floatrow}
- \usepackage{textcomp}
- \usepackage{microtype}
- '\renewcommand{\sectionmark}[1]{\markright{\thesection~- ~#1}}'
- \renewcommand{\rmdefault}{pplx}
- \newcommand{\E}{\operatorname{E}}
- \newcommand{\V}{\operatorname{V}}
- \renewcommand{\Pr}{\operatorname{P}}
KOMAoptions:
- footsepline = true
- headsepline = true
automark: yes
caption:
- labelfont=bf
- labelsep=period
- font=small
header:
- pos: ro
  next: Robin Baudisch, Merlin Kopfmann, Maximilian Neudert
---

<style type="text/css">
body{
  font-size: 12px;
}
h1 {
  font-size: 18px;
}
h1 {
  font-size: 14px;
}
h1 {
  font-size: 12px;
}
</style>

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=TRUE,     # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 10,     # set figure width
                      out.width = "100%", # set width of displayed images
                      warning=FALSE,      # show R warnings
                      message=FALSE)     # show R messages
```

<!---** Hochschule Darmstadt | Studiengang Data Science | Sommersemester 2019 **--->

\begin{titlepage}
	\begin{center}
		\null\vfill
		{\Huge Computational Statistics}\vfill
		{\fontsize{16 pt}{0 mm} {\fontfamily{pzc}\selectfont Robin Baudisch}}\\
		{\fontsize{16 pt}{0 mm} {\fontfamily{pzc}\selectfont Merlin Kopfmann}}\\
		{\fontsize{16 pt}{0 mm} {\fontfamily{pzc}\selectfont Maximilian Neudert}}\\[12pt]
		{\fontsize{12 pt}{0 mm} {\fontfamily{pzc}\selectfont\today}}
	\end{center}
\end{titlepage}
\cleardoublepage

\thispagestyle{empty}
\tableofcontents
\cleardoublepage

Alle Aufgaben werden mit folgendem Seed bearbeitet:

```{r}
set.seed(42)
```

Und es werden folgende Libraries benutzt:

```{r}
usepackage = function(name) {
  x = deparse(substitute(name))
  if (!require(x,character.only = TRUE)) {
    install.packages(x,dep=TRUE)
    if(!require(x,character.only = TRUE)) stop("Package not found")
  }
}
usepackage(knitr)
usepackage(ggplot2)
usepackage(tidyr)
usepackage(lm.beta)
usepackage(car)
usepackage(magrittr)
usepackage(boot)
```

\cleardoublepage

# Einführung

## A1

```{r}
# Lade Daten
load(file='res/Donald.RData')
data <- Donald_1

# Fitte die Regression 
fit <- lm(
  Trump ~ Alter + Geschlecht + Minderheit + Fremdenfeindlich + IQ,
  data = data
)

# Ergebnis
summary(fit)
```

## A2

```{r, include=TRUE, message=FALSE}
# standardisiere die Parameter des Regressionsmodells
fit.beta <- lm.beta(fit)
print(fit.beta)
```

Der Parameter "Fremdenfeindlich" hat den größten Effekt auf die abhängige Variable. 
Je höher der Parameterwert, desto größer die Zustimmung zu Trump (in \%). Der Parameter "IQ" hat einen moderaten negativen Effekt auf die Ausprägung der abhängigen Variable.
Je höher der Parameterwert, desto geringer die Zustimmung zu Trump (in \%). DIe Parameter "Geschlecht", "Minderheit" und "Alter" haben jeweils einen geringen Effekt auf die Ausprägung der abhängigen Variable.

## A3

```{r, include=TRUE, message=FALSE}
KI <- confint(object = fit, level = 0.95)
kable(KI, format = 'pandoc', align = 'c', digits = 3)
```

In der Ausgabe sehen wir die Intervallgrenzen für das 95\%-Konfidenzintervall.

## A4

```{r, include=TRUE,  message=FALSE}
vif_fit <- vif(mod = fit)
vif_fit
```

Die VIF-Werte liegen alle deutlich unter 5(10), es liegen also keine Hinweise für Multikolliniarit?t zwischen den Modellparametern vor.

## A5

```{r, include=TRUE,  message=FALSE, fig.height=8}
par(mfrow = c(2, 2)) 
plot(fit)
```
```{r}
data$predicted <- predict(fit)
data$residuals <- residuals(fit)

data %>% 
  gather(key = "iv", value = "x", -Trump, -predicted, -residuals, -Minderheit, -Geschlecht) %>%
  ggplot(aes(x = x, y = Trump)) +
  geom_segment(aes(xend = x, yend = predicted), alpha = .2) +
  geom_point(aes(color = factor(residuals))) +
  guides(color = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  facet_grid(~ iv, scales = 'free_x') +
  theme_bw()
```

Die Residuenanalyse ergibt, dass die Residuen näherungsweise normalverteilt sind und es keine klar erkennbaren Muster in den Residuenplots gibt. 

Zudem wurden die Ausprägungen der (nicht-binären) unabhängigen Parameter den Ausprägungen der abhängigen Variablen durch Scatterplots gegenübergestellt.
Lediglich der Parameter "Fremdenfeindlich" weist einen klar erkennbaren linearen Zusammenhang zur abhängigen Variabel auf. Dies bestätigt das Ergebnis aus 2.

```{r}
avPlot(fit, "Fremdenfeindlich")
avPlot(fit, "Alter")
avPlot(fit, "Minderheit")
avPlot(fit, "IQ")
avPlot(fit, "Geschlecht")
```

An den Added-Variable Plot wird nochmals deutlich, dass "Fremdenfeindlich", "IQ" und "Alter" einen linearen Trend aufweisen. Die beiden binären Variablen "Minderheit" und "Geschlecht" kann man nicht linear erklären.

## A6

```{r, include=TRUE, message=FALSE}
my_data <- data.frame("Geschlecht" = 1, "Alter" = 24, 
                      "Minderheit" = 0, "Fremdenfeindlich" = 5, "IQ" = 100)
my_data$predicted <- predict(fit, newdata =  my_data)


my_data2 <- data.frame("Geschlecht" = 1, "Alter" = 27, 
                      "Minderheit" = 1, "Fremdenfeindlich" = 3, "IQ" = 100)
my_data2$predicted <- predict(fit, newdata =  my_data2)


my_data3 <- data.frame("Geschlecht" = 1, "Alter" = 29, 
                      "Minderheit" = 0, "Fremdenfeindlich" = 8, "IQ" = 90)
my_data3$predicted <- predict(fit, newdata =  my_data3)

pred_sum <- rbind(my_data, my_data2, my_data3)
pred_sum
```

Das Modell prognostiziert uns unterschiedliche Ergebnisse. Man erkennt klar, dass "Fremdenfeindlich" die größte Auswirkung auf die Zustimmungsrate hat.

\cleardoublepage

# Lineare Regression

## A1

```{r}
load(file = "res/Donald.RData")

## Vergleich der Variablenverteilung zwischen Trainings- und Testdatensatz/Lineare Regression (Drei Mal)
for (i in 1:3){ 
smp_size <- 100
train_ind <- sample(seq_len(nrow(Donald_1)), size = smp_size)

train <- Donald_1[train_ind, ]
test <- Donald_1[-train_ind, ]
print(i)
cat("train: \n\n")
print(summary(train))
cat("\n")
cat("test: \n\n")
print(summary(test))
linreg <- lm(Trump ~ Alter + Geschlecht + Minderheit + Fremdenfeindlich + IQ, 
             data = train)
pred <- predict.lm(linreg, test)
cat("\n")
cat("MSE: \n")
print(mean((test$Trump - pred) ^ 2))
cat("_______________________________ \n\n")
}

```

Durch die zufällige Aufteilung der Datenpunkte in zwei disjunkte Datensätze (train, test) entstehen Diskrepanzen zwischen den Verteilungen der Variablen. Diese Schwankungen wirken sich dann auch auf die Modellgüte aus. 
Wir wissen aus der vergangenen Übung, dass das Merkmal "Fremdenfeinlich" die Kovariate mit dem stärksten Einfluss auf die abhängige Variable ist. Vergleichen wir die Verteilung dieses Merkmals zwischen Train- und Testdatensatz in 1 mit dem aus 3, fällt auf, dass die Diskrepanzen zwischen train und test in 3 eindeutig stärker ausfallen, als in 1. Dies spiegelt sich im jeweiligen MSE wieder: Das Modell, welches mit den Daten aus 1 trainiert und getestet wurde, weist einen deutlich niedrigeren MSE auf, als das Modell aus 3.

\cleardoublepage

## A2

\subsubsection*{a)}

```{r}
mse <- c()
for (i in 1:nrow(Donald_1)){
  lou <- lm(Trump ~ Alter + Geschlecht + Minderheit + Fremdenfeindlich + IQ, 
            data = Donald_1[-i,])
  pred <- predict.lm(lou, Donald_1[i,])
  mse[i] <- mean((Donald_1[i,]$Trump - pred) ^ 2)
}

mse <- sum(mse)/nrow(Donald_1)
kable(mse, format = 'pandoc', align = 'c', digits = 3)
```

Der MSE für die Leave-one-out Cross-Validation liegt zwischen den berechneten MSE's aus Aufgabe 1. Dies ist zu erwarten, da wir hier untersuchen, wie gut das lineare Regressionsmodell auf "ungesehenen" Daten performt. In dem wir jeweils nur einen Datenpunkt als "Test"-Datensatz verwenden, bekommen wir so viele MSE-Werte, wie Datenpunkte im Datensatz. Diese MSE-Werte werden gemittelt, um eine "robustere" Einschätzung über die Generalisierbarkeit des Modells auf unsere Daten zu erhalten.

\subsubsection*{b)}

```{r}
model <- glm(
  Trump ~ Alter + Geschlecht + Minderheit + Fremdenfeindlich + IQ,
  data = Donald_1
)

cv_model <- cv.glm(Donald_1, model, K = nrow(Donald_1))

mse1 <- cv_model$delta
kable(mse1, format = 'pandoc', align = 'c', digits = 3)
```

Die Implementierung des "Leave-one-out cross-validation"-Verfahrens durch cv.glm gibt zwei Modellgütemetriken zurück. Der erste Wert ist der durschnittliche MSE über die Zeilen. Dieser ist identisch mit dem Wert aus a). Man kann vermuten, dass die Implementierung des "Leave-one-out cross-validation"-Verfahrens durch cv.glm identisch mit unserer manuellen Implementierung aus a) ist. Der zweite Wert ist laut Dokumentation ein Bias-korrigierter MSE. 

## A3

```{r}
mse <- matrix(NA, nrow = 20, ncol = 3)
for (k in c(5,10)){
  for (i in 1:10){
    mse[ifelse(k == 5, i, i + 10),] <- c(i, cv.glm(Donald_1, model, K = k)$delta[1], k)
  }
}

mse <- data.frame(mse)
names(mse) <- c("index", "mse", "k")
mse$k <- as.character(mse$k)

gg = ggplot(
data = mse,
  mapping = aes(
    x = index,
    y = mse,
    color = k
  )
)
gg = gg + xlab("Iteration")
gg = gg + scale_x_continuous(breaks = c(1,5,10))
gg + geom_line()
```

Bei der $k$-fachen $k$reuzvalidierung wird die ursprüngliche Stichprobe zufällig in $k$ gleich große Teilstichproben aufgeteilt. Von den $k$ Teilstichproben wird eine einzige Teilstichprobe als Validierungsdaten für den Test des Modells aufbewahrt und die restlichen $k - 1$ Teilstichproben werden als Trainingsdaten verwendet. Der $k$reuzValidierungsprozess wird dann $k$-mal wiederholt, wobei jede der $k$ Teilstichproben genau einmal als Validierungsdaten verwendet wird. Die $k$-Ergebnisse $k$önnen dann gemittelt werden, um eine einzige Schätzung zu erhalten. Der Vorteil dieser Methode gegenüber des wiederholt zufälligen Subsampling besteht darin, dass alle Beobachtungen sowohl für das Training als auch für die Validierung verwendet werden und jede Beobachtung genau einmal für die Validierung verwendet wird. Die $k$-fache $k$reuzvalidierung mit $k = 10$ wird häufig verwendet, aber im Allgemeinen bleibt $k$ ein nicht fixierter Parameter.

Untereinander schwanken die MSE-Werte zufällig. Als Trend ist zu beobachten, dass für $k = 5$ die MSE-Werte niedriger sind. Alles in allem variieren die MSE-Werte um einen Wert von $38$. Dies deckt sich mit den Ergebnissen aus Aufgabe 2. Die Schwankungen sind ebenfalls im Bereich der generierten MSE-Werte aus aus Aufgabe 1. Interessant ist, dass wir in Aufgabe 1 mit $\operatorname{MSE} \sim 35$ einen, im Vergleich, relativ niedrigen MSE-Wert erzielt haben.

\cleardoublepage

# Cross Validation

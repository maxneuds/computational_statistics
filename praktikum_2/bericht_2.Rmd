---
output:
  pdf_document: default
  html_document: default
header-includes: 
    - \usepackage{amsthm}
    - \usepackage{xcolor}
documentclass: article
<!---output: beamer_presentation--->
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=TRUE,     # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 10,     # set figure width
                      out.width = "100%", # set width of displayed images
                      tidy = TRUE,
                      warning=TRUE,      # show R warnings
                      message=FALSE)     # show R messages

packageTest<- function(x)  {
    if (!require(x,character.only = TRUE))  {
      install.packages(x,dep=TRUE)
      if(!require(x,character.only = TRUE)) stop("Package not found")
    }
}

options(warn=-1)
```


<!--- Solution Region --->
<style>
#solution {
  background-color: #8FBC8F;
  border-style: solid;
  border-color: blue;
  margin-left: 20px;
  margin-bottom: 15px;
  padding: 5px;
}
</style>



<!---**Sommersemester 2019 |Studiengang Data Science | Hochschule Darmstadt **--->




\theoremstyle{break}
\newtheorem{auf}{Aufgabe}

\newcommand{\R}{{\sffamily R} }

\begin{centering}
%\vspace{-2 cm}
\Huge
{\bf Übung 2}\\
\Large
Computational Statistics\\
\normalsize
Sommersemester 2019\\
April 15, 2019\\
J. Groos (FBMN, h\_da)\\
\end{centering}


\hrulefill

**Name:**

\hrulefill


## A 1

```{r}
set.seed(42)
load(file = "Donald.RData") # importiere Datensatz

## Vergleich der Variablenverteilung zwischen Trainings- und Testdatensatz/Lineare Regression (Drei Mal)
for (i in 1:3){ 
smp_size <- 100
train_ind <- sample(seq_len(nrow(Donald_1)), size = smp_size)

train <- Donald_1[train_ind, ]
test <- Donald_1[-train_ind, ]
print(i)
cat("train: \n\n")
print(summary(train))
cat("\n")
cat("test: \n\n")
print(summary(test))
linreg <- lm(Trump ~ Alter + Geschlecht + Minderheit + Fremdenfeindlich + IQ, 
             data = train)
pred <- predict.lm(linreg, test)
cat("\n")
cat("MSE: \n")
print(mean((test$Trump - pred) ^ 2))
cat("_______________________________ \n\n")
}

```
Durch die zufällige Aufteilung der Datenpunkte in zwei disjunkte Datensätze (train, test) entstehen Diskrepanzen zwischen den Verteilungen der Variablen. Diese Schwankungen wirken sich dann auch auf die Modellgüte aus. 
Wir wissen aus der vergangenen Übung, dass das Merkmal "Fremdenfeinlich" die Kovariate mit dem stärksten Einfluss auf die abhängige Variable ist. Vergleichen wir die Verteilung dieses Merkmals zwischen Train- und Testdatensatz in 1 mit dem aus 3, fällt auf, dass die Diskrepanzen zwischen train und test in 3 eindeutig stärker ausfallen, als in 1. Dies spiegelt sich im jeweiligen MSE wieder: Das Modell, welches mit den Daten aus 1 trainiert und getestet wurde, weist einen deutlich niedrigeren MSE auf, als das Modell aus 3.

## A 2
### a)
```{r}
mse <- c()
for (i in 1:nrow(Donald_1)){
  lou <- lm(Trump ~ Alter + Geschlecht + Minderheit + Fremdenfeindlich + IQ, 
            data = Donald_1[-i,])
  pred <- predict.lm(lou, Donald_1[i,])
  mse[i] <- mean((Donald_1[i,]$Trump - pred) ^ 2)
}

mse <- sum(mse)/nrow(Donald_1)
mse
```
Der MSE für die Leave-one-out Cross-Validation liegt zwischen den berechneten MSE's aus Aufgabe 1. Dies ist zu erwarten, da wir hier untersuchen, wie gut das lineare Regressionsmodell auf "ungesehenen" Daten performt. In dem wir jeweils nur einen Datenpunkt als "Test"-Datensatz verwenden, bekommen wir so viele MSE-Werte, wie Datenpunkte im Datensatz. Diese MSE-Werte werden gemittelt, um eine "robustere" Einschätzung über die Generalisierbarkeit des Modells auf unsere Daten zu erhalten.

### b)
```{r}
packageTest('boot')

model <- glm(Trump ~ Alter + Geschlecht + Minderheit + Fremdenfeindlich + IQ, 
             data = Donald_1)

cv_model <- cv.glm(Donald_1, model, K = nrow(Donald_1))

mse1 <- cv_model$delta
mse1
```
Die Implementierung des "Leave-one-out cross-validation"-Verfahrens durch cv.glm gibt zwei Modellgütemetriken zurück. Der erste Wert ist der durschnittliche MSE über die Zeilen. Dieser ist identisch mit dem Wert aus a). Man kann vermuten, dass die Implementierung des "Leave-one-out cross-validation"-Verfahrens durch cv.glm identisch mit unserer manuellen Implementierung aus a) ist. Der zweite Wert ist laut Dokumentation ein Bias-korrigierter MSE. 

## A 3
```{r}
packageTest("ggplot2")
set.seed(42)

mse <- matrix(NA, nrow = 20, ncol = 3)
for (k in c(5,10)){
  for (i in 1:10){
    mse[ifelse(k == 5, i, i + 10),] <- c(i, cv.glm(Donald_1, model, K = k)$delta[1], k)
  }
}

mse <- data.frame(mse)
names(mse) <- c("index", "mse", "k")
mse$k <- as.character(mse$k)

gg = ggplot(
data = mse,
  mapping = aes(
    x = index,
    y = mse,
    color = k
  )
)
gg = gg + xlab("Iteration")
gg = gg + scale_x_continuous(breaks = c(1,5,10))
gg + geom_line()
```
Bei der $k$-fachen $k$reuzvalidierung wird die ursprüngliche Stichprobe zufällig in $k$ gleich große Teilstichproben aufgeteilt. Von den $k$ Teilstichproben wird eine einzige Teilstichprobe als Validierungsdaten für den Test des Modells aufbewahrt und die restlichen $k - 1$ Teilstichproben werden als Trainingsdaten verwendet. Der $k$reuzValidierungsprozess wird dann $k$-mal wiederholt, wobei jede der $k$ Teilstichproben genau einmal als Validierungsdaten verwendet wird. Die $k$-Ergebnisse $k$önnen dann gemittelt werden, um eine einzige Schätzung zu erhalten. Der Vorteil dieser Methode gegenüber des wiederholt zufälligen Subsampling besteht darin, dass alle Beobachtungen sowohl für das Training als auch für die Validierung verwendet werden und jede Beobachtung genau einmal für die Validierung verwendet wird. Die $k$-fache $k$reuzvalidierung mit $k = 10$ wird häufig verwendet, aber im Allgemeinen bleibt $k$ ein nicht fixierter Parameter.

Untereinander schwanken die MSE-Werte zufällig. Als Trend ist zu beobachten, dass für $k = 5$ die MSE-Werte niedriger sind. Alles in allem variieren die MSE-Werte um einen Wert von $38$. Dies deckt sich mit den Ergebnissen aus Aufgabe 2. Die Schwankungen sind ebenfalls im Bereich der generierten MSE-Werte aus aus Aufgabe 1. Interessant ist, dass wir in Aufgabe 1 mit $\operatorname{MSE} \sim 35$ einen, im Vergleich, relativ niedrigen MSE-Wert erzielt haben.

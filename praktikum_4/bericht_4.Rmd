---
output:
  pdf_document: default
  html_document: default
header-includes: 
    - \usepackage{amsthm}
    - \usepackage{xcolor}
documentclass: article
<!---output: beamer_presentation--->
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=TRUE,     # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 10,     # set figure width
                      out.width = "100%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=FALSE)     # show R messages
packageTest<- function(x)  {
    if (!require(x,character.only = TRUE))  {
      install.packages(x,dep=TRUE)
      if(!require(x,character.only = TRUE)) stop("Package not found")
    }
}
options(warn=-1)
```


<!--- Solution Region --->
<style>
#solution {
  background-color: #8FBC8F;
  border-style: solid;
  border-color: blue;
  margin-left: 20px;
  margin-bottom: 15px;
  padding: 5px;
}
</style>



<!---**Sommersemester 2019 |Studiengang Data Science | Hochschule Darmstadt **--->




\theoremstyle{break}
\newtheorem{auf}{Aufgabe}

\newcommand{\R}{{\sffamily R} }

\begin{centering}
%\vspace{-2 cm}
\Huge
{\bf Uebung 4: Shrinkage}\\
\Large
Computational Statistics\\
\normalsize
Sommersemester 2019\\
April 15, 2019\\
J. Groos (FBMN, h\_da)\\
\end{centering}


\hrulefill

**Name:**

\hrulefill

# Aufgabe 1
### a)

```{r}
packageTest("ElemStatLearn")

linreg <- lm(lpsa~.-train, data = prostate)
linreg_coeff <- linreg$coefficients
```

### b)
```{r}
packageTest("glmnet")

ridgereg_0 <- glmnet(x = model.matrix(linreg), y = prostate$lpsa, alpha = 0, lambda = 0) 
ridgereg_0_coeff = as.data.frame(as.matrix(ridgereg_0$beta))

ridgereg_10 <- glmnet(x = model.matrix(linreg), y = prostate$lpsa, alpha = 0, lambda = 10) 
ridgereg_10_coeff = as.data.frame(as.matrix(ridgereg_10$beta))

ridgereg_coeffs <- data.frame(ridgereg_0_coeff, ridgereg_10_coeff, linreg_coeff)
colnames(ridgereg_coeffs) <-  c("lambda_0", "lambda_10", "linreg")
ridgereg_coeffs

lambda_0_mean <- mean(ridgereg_coeffs$lambda_0)
lambda_10_mean <- mean(ridgereg_coeffs$lambda_10)
```

Man sieht deutlich, dass ein größeres $\lambda$ die Koeffizienten stärker "schrumpft" als ein kleines $\lambda$ (Mittelwert der Koeffizienten für $\lambda$ = 0: `r lambda_0_mean`; für $\lambda$ = 10: `r lambda_10_mean`). Bei $\lambda$ = 0 sind die Koeffizienten (bis auf den Intercept) nahezu identisch mit den geschätzten Koeffizienten des linearen Regressionsmodells. Im Falle $\lambda$ = 10 sind zudem alle Koeffizienten positiv.


### c)
```{r}
packageTest("ggplot2")
packageTest("reshape")
packageTest("gridExtra")
packageTest("grid")

coeff_plot <- function(lambda_list, y, linear_model, alpha){
  coeff_list <- matrix(NA, nrow = length(names(linear_model$coefficients)),
                       ncol = length(lambda_list))
  
  for (i in 1:length(lambda_list)){
    ridgereg <- glmnet(x = model.matrix(linear_model), y = y, 
                       alpha = alpha, lambda = lambda_list[i])
    coeff <- as.vector(ridgereg$beta)
    coeff_list[,i] <- coeff
  }
  coeff_list <- as.data.frame(coeff_list)
  colnames(coeff_list) <- lambda_list
  coeff_list$names <- names(linear_model$coefficients)
  return(coeff_list)
}

coeffs <- coeff_plot(lambda_list = c(0, 2, 10, 50, 100), y = prostate$lpsa, 
                     linear_model = linreg, alpha = 0) 

df1.m <- melt(coeffs,id.vars = "names")

p <-   ggplot(df1.m, aes(x=variable, y=value, fill=names)) + 
  geom_bar(stat="identity", width = 0.75) +
    facet_grid(. ~ names) + ggtitle("Beta-Schätzer in Abhängigkeit verschiedener Lambdas")

p
```
Negative $\beta$ werden mit größer werdendem $\lambda$ positiv. Außerdem konvergieren die $\beta$-Schätzer Richtung 0, werden aber nie genau 0 (anders als beim Lasso-Verfahren).

### d)
```{r}
train = prostate[prostate$train == TRUE, ]
test = prostate[prostate$train == FALSE, ]

lambdas <- c(0, 0.09, 2)
for (lambda in lambdas){
    ridgereg <- glmnet(x = as.matrix(within(train, rm(lpsa))), y = train$lpsa, alpha = 0, 
                       lambda = lambda)
    y.true <- test$lpsa
    y.pred <- predict.glmnet(ridgereg, newx = as.matrix(within(test, rm(lpsa))), 
                             lambda = lambda, alpha = 0)
    cat('\n')
    print(lambda) 
    cat(mean(y.true-y.pred)^2)
    cat('\n')
}
```

Das optimale $\lambda$ liegt im Bereich um 0.09.

### e)
```{r}
lambdas <- seq(0, 2, 0.01)
cv_glm_fit <- cv.glmnet(x = as.matrix(within(prostate, rm(lpsa))), y = prostate$lpsa, 
                        alpha = 0, lambda = lambdas)
opt_lambda <- cv_glm_fit$lambda.min

mse <- c()
for (lambda in lambdas){
    ridgereg <- glmnet(x = as.matrix(within(train, rm(lpsa))), y = train$lpsa, alpha = 0, 
                       lambda = lambda)
    y.true <- test$lpsa
    y.pred <- predict.glmnet(ridgereg, newx = as.matrix(within(test, rm(lpsa))), 
                                       lambda = lambda, alpha = 0)
    mse <- append(mse, mean(y.true-y.pred)^2)
}

lambda_mse <- NULL
lambda_mse$lambdas <- lambdas
lambda_mse$mse <- mse

lambda_mse <- as.data.frame(lambda_mse)
ggplot(lambda_mse, aes(x=lambdas, y=mse)) +
    geom_point(shape=1) + 
  ggtitle("Test-MSE in Abhängigkeit von verschiedenen Lambda-Werten")
opt_lambda_test <- lambda_mse$lambdas[lambda_mse$mse == min(lambda_mse$mse)]
```

Der optimale Schätzer für $\lambda$ des kompletten Datensatzes beträgt laut cv.glmnet `r opt_lambda`. Dieser liegt nicht weit vom Lambda-Wert aus d) mit dem niedrigsten Test-MSE (0.09). Auch der Plot des Test-MSE in Abhängigkeit von verschiedenen $\lambda$-Werten spricht für ein optimales Lambdas von ~0.1. (Optimales Lambda (out-of-sample) = `r opt_lambda_test`)


### f)
```{r}
linreg <- lm(lpsa~.-train, data=prostate)

ridgereg <- glmnet(x = model.matrix(linreg), y = prostate$lpsa, alpha = 0, 
                       lambda = opt_lambda)

ridgereg_coeff <- as.vector(ridgereg$beta)
coeff_df <- NULL
coeff_df$ridgereg_coeffs <- ridgereg_coeff
coeff_df$linreg_coeffs <- linreg_coeff
coeff_df <- as.data.frame(coeff_df)

coeff_df
```

Vergleicht man die Koeffizienten der Ridge-Regression mit den Koeffizienten der linearen Regression as a) wird der "Shrinking"-Effekt der Ridge-Regression deutlich. Nahezu alle Koeffizienten der Ridge-Regression liegen näher an der 0 als die Koeffizienten der linearen Regression (mit Ausnahme des Koeffizienten der Kovariaten "gleason").

# Aufgabe 2
### a)

```{r}
packageTest("tidyverse")
lasso_0 <- glmnet(x = model.matrix(linreg), y = prostate$lpsa, alpha = 1, 
                       lambda = 0)
lasso_10 <- glmnet(x = model.matrix(linreg), y = prostate$lpsa, alpha = 1, 
                       lambda = 10)

coeff_df$lasso_0_coeffs <- as.vector(lasso_0$beta)

coeff_df$lasso_10_coeffs <- as.vector(lasso_10$beta)

coeff_df %>% select(2:4)
```

Das Lasso-Verfahren mit $\lambda$ = 0 generiert nahezu identische $\beta$-Schätzer wie die lineare Regression. Bei $\lambda$ = 10 liegen alle "geschrumpften" $\beta$-Schätzer bereits bei 0. 

### b)
```{r}
coeffs <- coeff_plot(lambda_list = c(0, 0.01, 0.1, 1), y = prostate$lpsa, 
                     linear_model = linreg, alpha = 1) 

df1.m <- melt(coeffs,id.vars = "names")

p <-   ggplot(df1.m, aes(x=variable, y=value, fill=names)) + 
  geom_bar(stat="identity", width = 0.75) +
    facet_grid(. ~ names) + ggtitle("Beta-Schätzer in Abhängigkeit verschiedener Lambdas")

p
```

Auch beim Lasso-Verfahren zeichnet sich ein ähnliches Bild ab wie bei Aufgabe 1 c). Bei größer werdendem $\lambda$ "schrumpfen" die $\beta$-Schätzer, konvergieren gegen 0 und werden, anders als bei Ridge-Regression, ab einem bestimmten $\lambda$ sogar 0. 

### c)
```{r}
lambdas <- c(0, 0.002, 1)
for (lambda in lambdas){
    ridgereg <- glmnet(x = as.matrix(within(train, rm(lpsa))), y = train$lpsa, alpha = 1, 
                       lambda = lambda)
    y.true <- test$lpsa
    y.pred <- predict.glmnet(ridgereg, newx = as.matrix(within(test, rm(lpsa))), 
                             lambda = lambda, alpha = 1)
    cat('\n')
    print(lambda) 
    cat(mean(y.true-y.pred)^2)
    cat('\n')
}
```

Hier scheint der "Sweet Spot" für $\lambda$ zwischen 0.002 und 1 zu liegen.


### d)
```{r}
lambdas <- seq(0, 1, 0.01)
cv_glm_fit <- cv.glmnet(x = as.matrix(within(prostate, rm(lpsa))), y = prostate$lpsa, 
                        alpha = 1, lambda = lambdas)
opt_lambda <- cv_glm_fit$lambda.min

mse <- c()
for (lambda in lambdas){
    ridgereg <- glmnet(x = as.matrix(within(train, rm(lpsa))), y = train$lpsa, alpha = 1, 
                       lambda = lambda)
    y.true <- test$lpsa
    y.pred <- predict.glmnet(ridgereg, newx = as.matrix(within(test, rm(lpsa))), 
                                       lambda = lambda, alpha = 1)
    mse <- append(mse, mean(y.true-y.pred)^2)
}

lambda_mse <- NULL
lambda_mse$lambdas <- lambdas
lambda_mse$mse <- mse

lambda_mse <- as.data.frame(lambda_mse)
ggplot(lambda_mse, aes(x=lambdas, y=mse)) +
    geom_point(shape=1) + 
    ggtitle("Test-MSE in Abhängigkeit von verschiedenen Lambda-Werten")
opt_lambda_test <- lambda_mse$lambdas[lambda_mse$mse == min(lambda_mse$mse)]
```

Hier liegt der optimale Wert für $\lambda$ für den kompletten Datensatz bei `r opt_lambda`. Dieser Wert liegt wieder sehr nahe am ermittelten Optimum aus c). Plottet man den Test-MSE (out-of-sample) in Abhängigkeit verschiedener $\lambda$-Werte, kommt man allerdings auf ein ein optimalen $\lambda$-Wert von `r opt_lambda_test`.

### e)
```{r}
lasso <- glmnet(x = model.matrix(linreg), y = prostate$lpsa, alpha = 1, 
                       lambda = opt_lambda)

lasso_coeff <- as.vector(lasso$beta)
coeff_df$lasso_coeff <- lasso_coeff

coeff_df %>% select(2, 5)
```

Wie bereits in Aufgabe 1 ist beim Lasso-Verfahren gut zu erkennen, dass die Koeffizienten im Vergleich zur linearen Regression aus Aufgabe 1 a) "geschrumpft" werden und näher an der 0 liegen. 
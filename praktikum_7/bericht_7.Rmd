---
output: pdf_document
header-includes: 
- \usepackage[utf8]{inputenc}
- \usepackage[T1]{fontenc}
- \usepackage[ngerman]{babel}
- \usepackage{amsmath,amssymb,amsthm}
- \usepackage{dsfont}
- \usepackage{listings}
- \usepackage{floatrow}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- '\fancyhead[C,C]{Robin Baudisch, Merlin Kopfmann, Maximilian Neudert}'
- \fancyhead[L]{}
- \fancyhead[R]{}
- \fancyfoot[C,C]{\thepage}
- \renewcommand{\footrulewidth}{0.4pt}
- \newcommand{\E}{\operatorname{E}}
- \newcommand{\V}{\operatorname{V}}
- \renewcommand{\Pr}{\operatorname{P}}
---

<style type="text/css">
body{
  font-size: 12px;
}
h1 {
  font-size: 18px;
}
h1 {
  font-size: 14px;
}
h1 {
  font-size: 12px;
}
</style>

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=TRUE,     # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 10,     # set figure width
                      out.width = "100%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=FALSE)     # show R messages
```

<!---** Hochschule Darmstadt | Studiengang Data Science | Sommersemester 2019 **--->

```{r, echo=FALSE}
set.seed(42)
library(knitr)
library(ggplot2)
library(tidyr)
```

# Monte-Carlo-Simulation

## Aufgabe 1

```{r}
zi = function(xi, yi) {
  out = 4 * ifelse(xi^2 + yi^2 <= 1, 1, 0)
  return(out)
}

simpi = function(n) {
  xi = runif(n, -1, 1)
  yi = runif(n, -1, 1)
  z = zi(xi, yi)
  out = mean(z)
  return(out)
}
```

```{r}
pimat = matrix(NA, nrow = 8, ncol = 3)
for (k in seq(2)) {
  z = simpi(10^k)
  pimat[k, 1] = k
  pimat[k, 2] = z
  pimat[k, 3] = round(abs(pi - z), 6)
}
df_pi = data.frame(pimat)
colnames(df_pi) = c("k", "z", "e")
kable(df_pi, align = "c")
```

Um mit Chebyshev abschätzen zu können benötigen wir Erwartungswert und Varianz der Zufallsvariablen $z_i$.
Der Erwartungswert sollte mit $\pi$ übereinstimmen. Wir rechnen nach. Sei dazu $z_1$ die Approximation von $\pi$ mittels des Einheitskreises im Einheitsquadrat eingeschränkt auf den 1. Quadranten.

\begin{align}
\E(z_i)
& = \E(4 z_1)
= 4 \E(z_i)
= 4 \int_0^1 \sqrt{1 - x^2}\ dx = 4 \frac \pi 4 = \pi
\end{align}

\begin{align}
\V(z_i)
& = \V(4 z_1)
= 16 \E(z_i)\\
& = 16 \int_0^1 \sqrt{1 - x^2}^2\ dx - 16 \left(  \int_0^1 \sqrt{1 - x^2}\ dx\right)^2\\
& = 16 \int_0^1 1-x^2\ dx - 16 \frac{\pi^2}{16}
= 16 \left[ x - \frac 1 3 x^3 \right]_0^1 - \pi^2\\
& = \frac{32}{3} - \pi^2 \approx 0.797
\end{align}

Damit erhalten wir dann für $\bar z$:

\begin{align}
& \E(\bar z)
= \E\left(\frac 1 n \sum_{i=1}^n z_i \right)
= \frac 1 n n \E(z_i) = \E(z_i) = \pi\\
& \V(\bar z)
= \V \left(\frac 1 n \sum_{i=1}^n z_i \right)
= \frac{1}{n^2} \left(\sum_{i=1}^n z_i \right)
= \frac{n}{n^2} \V(z_i)
= \frac{1}{n} \V(z_i) \approx \frac{0.797}{n}
\end{align}

Nach Chebyshev gilt nun für einen beliebigen Fehler $\varepsilon$ und $\sigma^2 = \V(z_i)$:

\begin{align}
& \Pr\left( \vert \bar z - \pi \vert \geq \varepsilon \right)
\leq \frac{\V(\bar z)}{\varepsilon^2} \\
\implies
& \Pr\left( \vert \bar z(n) - \pi \vert \geq \varepsilon \right)
\leq \frac{\sigma^2}{n^2 \varepsilon^2} = \frac{\sigma}{n \varepsilon}
\end{align}

Nun gilt es zu bedenken, dass die Abschätzung zwar für alle $\varepsilon > 0$ gilt, aber nur für $n \varepsilon^2 > \sigma^2$ wirklich Sinn macht. Sprich es macht nur Sinn zu prüfen, wie hoch die Chance ist, dass man ein Vielfaches der Standardabweichung vom Mittelwert abweicht.
Wie man an der Abschätzung erkennen kann, sinkt die Genauigkeit, dass wir um mehr als einen gegebenen Fehler abweichen mit zunehmenden $n$. Das heißt insbesondere, dass mit höheren $n$ der Fehler sinkt. Dies können wir anhand der Tabelle bestätigen.

## A2

### a)

```{r}
simsnd = function(n) {
  x = runif(n, -1.96, 1.96)
  phi = dnorm(x)
  out = 2 * 1.96 * mean(phi)
  return(out)
}
```

```{r}
sndmat = matrix(NA, nrow = 8, ncol = 3)
for (k in seq(6)) {
  z = simsnd(10^k)
  sndmat[k, 1] = k
  sndmat[k, 2] = z
  sndmat[k, 3] = round(abs(0.95 - z), 6)
}
df_snd = data.frame(sndmat)
colnames(df_snd) = c("k", "z", "e")
kable(df_snd, align="c")
```

### b)

```{r}
nexp = 10
M = matrix(NA, nrow = nexp, ncol = 4)
for (k in seq(5,7)) {
  for (i in seq(nexp)) {
    z = simsnd(10^k)
    M[i, 1] = i
    M[i,k - 3] = z
  }
}
df = data.frame(M)
colnames(df) = c("i", "5", "6", "7")
ggdf = gather(df, "k", "P", seq(2,4))
gg = ggplot(
  data = ggdf,
  mapping = aes(
    x = i,
    y = P,
    group = k,
    col = k
  )
)
gg = gg + xlab("Iteration") + ylab("P")
gg = gg + geom_point() 
gg = gg + stat_smooth(method = "gam", formula = y ~ s(x, bs='cr'), se = FALSE)
gg = gg + theme(
  axis.title.x=element_blank(),
  axis.text.x=element_blank(),
  axis.ticks.x=element_blank()
)
gg
```

In der Abbildung können wir erkennen, dass für $k = 5 \implies n = 10^5$ es noch eine signifikante Streuung um den erwarteten Wert von $P = 0.95$ gibt. Um dies graphisch besser darzustellen haben wir einen kubischen Regressionspline durch die Punkte gezogen. Man erkennt für $k = 5$ deutliche Abweichungen, während für $k = \lbrace 6,7 \rbrace$ der Spline eher eine Gerade am erwarteten Wert ist.
Erinnern wir uns an die Abschätzung mit Chebyshev aus Aufgabe 1, so sollten wir auch hier eine ähnliche Abschäzung erhalten (da analoges Vorgehen) und da die Wahrscheinlichkeit von Mittelwert abzuweichen mit dem Produkt aus Fehler und $n$ sinkt, braucht es schon ein hohes $n$, um möglichst kleine Fehler zu garantieren.
